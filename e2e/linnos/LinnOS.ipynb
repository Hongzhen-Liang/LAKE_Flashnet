{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing LinnOS End-to-end Workflow \n",
    "NOTE: Please read the README first before running the end-to-end workflows.\n",
    "\n",
    "This artifact is designed to be run on Chameleon testbed using Jupyter. It will run two end-to-end workflows on a Chameleon instance: baseline and LinnOS (more detail in README). Specifically, it will conduct the following steps:\n",
    "\n",
    "1. __Process IO traces to obtain Baseline results:__ <br>\n",
    "   a. Acquire an instance with a local SSD array from Chameleon <br>\n",
    "   b. Populate those drives with preprepared I/O trace dataset  <br>\n",
    "   c. Run the SSD replayer and obtain the baseline cdf results <br>\n",
    "2. __Train the LinnOS ML model and implant the learned weights to LinnOS kernel code :__ <br> \n",
    "   a. Train the LinnOS ML model using the output obtained from running the baseline and save the learned weights <br>\n",
    "   b. Use header generator to convert saved weight files to LinnOS kernel compatible headers (and put them inside LinnOS kernel source code) <br>\n",
    "3. __Install LinnOS kernel:__  <br>\n",
    "   a. Prepare the config file <br>\n",
    "   b. Install the required packages  <br>\n",
    "   c. Compile and reboot <br>\n",
    "4. __Using the LinnOS, replay the IO traces to obtain ML results:__   <br>\n",
    "    a. Run the SSD replayer (with LinnOS Kernel enabled) and obtain the linnos-ml cdf results <br>\n",
    "\n",
    "__Requirements:__ Chameleon account,familiarity with openstack and a storage hierarchy instance with Ubuntu 18.04 (or later)\n",
    "\n",
    "__Total run time of each step is shown below:__\n",
    "\n",
    "* __step 1 ----> Depending on the condition of the physical instance, runtime of this step can range from several minutes to several hours__\n",
    "* __step 2 ----> around 30 minutes__\n",
    "* __step 3 ----> around 30 minutes__\n",
    "* __step 4 ----> around 10 minutes__\n",
    "\n",
    "This package contains following files: LinnOS.ipynb, LinnOSResultsPlot.ipynb, LinnOSWriterReplayer.tgz, linux-5.4.8-linnos.tgz, reservation.sh, stack.yaml and References folder\n",
    "\n",
    "- LinnOS.ipynb (this script) is the main script that follows the steps mentioned above.\n",
    "- LinnOSResultsPlot.ipynb is the script that plots the end result of the LinnOS (i.e graph of baseline vs LinnOS-ML line). This script should run after LinnOS.ipynb is successfully executed.\n",
    "- LinnOSWriterReplayer.tgz contains LinnOS scripts used for populating drives, running SSD fail-over experiments and etc.\n",
    "- linux-5.4.8-linnos.tar.gz contains the LinnOS Kernel code.\n",
    "- reservation.sh and stack.yaml is used for creating a lease and an instance.\n",
    "- References folder that includes our recent results on Chameleon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters can be tuned based on users need by simply changing the string. \n",
    "# For example OS_PROJECT_NAME can be changed to match your project names.\n",
    "export OS_PROJECT_NAME=\"Chameleon Reproducibility Research\"\n",
    "export OS_REGION_NAME=\"CHI@TACC\"\n",
    "export NODE_TYPE=\"storage_hierarchy\"\n",
    "\n",
    "export RESOURCE_NAME=\"$USER-LinnOSStorage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This script creates/starts the lease, and exports a public IP address to access the instance after \n",
    "# it is created. If no available host error is obtained, it means that all available storage hierachy devices \n",
    "# are in use. Changing the start date of the reservation in reservation.sh script might solve this. \n",
    "# If the cell runs correctly then you will see \"Lease started successfully!\" message.\n",
    "\n",
    "source ./reservation.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Please proceed after the lease is successfully started.\n",
    "# If lease is successfully created then it takes roughly 10 minutes to iniate an instance.\n",
    "\n",
    "key_pair_upload\n",
    "\n",
    "stack_name=\"$RESOURCE_NAME\"\n",
    "\n",
    "openstack stack create \"$stack_name\" --wait \\\n",
    "  --template stack.yaml \\\n",
    "  --parameter floating_ip=\"$FLOATING_IP_ID\" \\\n",
    "  --parameter reservation_id=\"$RESERVATION_ID\" \\\n",
    "  --parameter key_name=\"$USER-jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat this cell until it returns success.\n",
    "wait_ssh \"$FLOATING_IP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TraceTag='trace'\n",
    "echo $FLOATING_IP\n",
    "export ConIP=$FLOATING_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Process IO traces to obtain Baseline results (Non-ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp LinnOSWriterReplayer.tgz cc@\"$ConIP\":/home/cc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" tar -xzf LinnOSWriterReplayer.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LinnOS requires three SSD drives to run failover behaviour. Here we list all the block devices and pick three SSD drives (for convenience we picked sde,sdf,sdg). (Notice that picking other drives require modifying the ml_model.h file inside the linux-5.4.8-linnos/block folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" lsblk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we are trying to get the baseline SSD performance results (i.e cdf graph). In order to achieve that, each picked drive needs to be populated with the I/O trace data. Note that each drive needs to be populated only once. Then we run SSD replayer to measure the performance of the three drive ssd group on populated I/O data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the following cell, we are populating the sde,sdf and sdg drives with anonymous.drive0,anonymous.drive1 and anonymous.drive2 I/O traces using the writer script. Writer script takes the ssd drive and I/O trace as arguments and populates the specified drive with the chosen trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input trace file format:\n",
    "# 1: timestamp in ms\n",
    "# 2: disk ID (not used)\n",
    "# 3: offset in bytes\n",
    "# 4: I/O size in bytes\n",
    "# 5: r/w type, 1 for read and 0 for write\n",
    "# Depending on the condition of the physical instance, runtime of this cell\n",
    "# can range from several minutes to several hours. \n",
    "# Currently we see writer return segmentation fault on Chameleon instances, but this does \n",
    "# not affect populating the drives with data, and you can proceed to the next cell.\n",
    "ssh cc@\"$ConIP\" << EOF\n",
    " cd LinnOSWriterReplayer\n",
    " nohup sudo ./writer /dev/sde 'testTraces/anonymous.drive0.'$TraceTag &\n",
    " nohup sudo ./writer /dev/sdf 'testTraces/anonymous.drive1.'$TraceTag &\n",
    " id_1=$(sudo pgrep -a writer | awk 'NR==1 {print $1}')\n",
    " id_2=$(sudo pgrep -a writer | awk 'NR==2 {print $1}')\n",
    " sudo ./writer /dev/sdg 'testTraces/anonymous.drive2.'$TraceTag\n",
    " wait \\$id_1\n",
    " wait \\$id_2\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the following cell we are using the replayer_fail script to obtain the results for the baseline model using the test IO set. (Training IO set is used to train the machine learning model and Test IO set is used for baseline vs ml linnOS comparison) Replayer_fail requires 3 SSD devices and their corresponding traces to demonstrate fail over behaviour (i.e if given IO request is costly, redirect it to other drives. The same logic applies to the other drives) if LinnOS kernel is installed. Without LinnOS kernel, Replayer_fail script basically just sends each trace to their corresponding drive. Replayer_fail script outputs the resultant latency values along with some other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The replayer output format is:\n",
    "# <Index of I/O>,<Index of device>,<schedule timestamp>,<I/O latency>,<I/O type>,<I/O size>,<I/O offset>,<submission timestamp>,<return state>\n",
    "# This process takes 3 minutes. If the cell successfully runs, you will see \"All done!\" message.\n",
    "ssh cc@\"$ConIP\" << EOF\n",
    " cd LinnOSWriterReplayer\n",
    " sudo ./replayer_fail /dev/sde-/dev/sdf-/dev/sdg \\\n",
    " 'testTraces/testdrive0.'$TraceTag \\\n",
    " 'testTraces/testdrive1.'$TraceTag \\\n",
    " 'testTraces/testdrive2.'$TraceTag py/TestTraceOutput\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing numpy dependency (for the percentile.py script)\n",
    "ssh cc@\"$ConIP\" pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Percentile.py takes the resultant latency values produced by replayer as input and calculates the cdf  to produce baseline trajectory in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" python3 LinnOSWriterReplayer/py/percentile.py 2 read \\\n",
    "LinnOSWriterReplayer/py/TestTraceOutput LinnOSWriterReplayer/py/BaselineData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here baseline performance ouput is saved to be used later in the LinnOSResultsPlot.ipynb script.\n",
    "scp cc@\"$ConIP\":/home/cc/LinnOSWriterReplayer/py/BaselineDataread_percentile.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Train the LinnOS ML model and implant the learned weights to LinnOS kernel code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell we are using the replayer_fail script to prepare our traces for linnOS (Training IO set is used to train the machine learning model and Test IO set is used for baseline vs linnOS-ml comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This process takes around 10 minutes.If the cell sucessfully runs, you will see \"All done!\" message.\n",
    "ssh cc@\"$ConIP\" << EOF\n",
    " cd LinnOSWriterReplayer\n",
    " sudo ./replayer_fail /dev/sde-/dev/sdf-/dev/sdg \\\n",
    " 'testTraces/traindrive0.'$TraceTag \\\n",
    " 'testTraces/traindrive1.'$TraceTag \\\n",
    " 'testTraces/traindrive2.'$TraceTag py/TrainTraceOutput\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp linux-5.4.8-linnos.tar.gz cc@\"$ConIP\":/home/cc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" tar -xf linux-5.4.8-linnos.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Installing dependencies (for the pred1.py script)\n",
    "ssh cc@\"$ConIP\" pip3 install --upgrade pip\n",
    "ssh cc@\"$ConIP\" pip3 install tensorflow==1.15.2\n",
    "ssh cc@\"$ConIP\" pip3 install keras==2.1.3 \n",
    "ssh cc@\"$ConIP\" pip3 install pandas\n",
    "ssh cc@\"$ConIP\" pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trace parser takes replayer output as input and converts it into ml-friendly dataset (i.e., ML trace dataset) which we will be using to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run-time is around couple minutes.\n",
    "for i in 0 1 2 \n",
    "do\n",
    "   ssh cc@\"$ConIP\" python3 LinnOSWriterReplayer/py/traceParser.py direct 3 4 \\\n",
    "   LinnOSWriterReplayer/py/TrainTraceOutput LinnOSWriterReplayer/mlData/temp1 \\\n",
    "   LinnOSWriterReplayer/mlData/\"mldrive${i}.csv\" \"$i\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we train the LinnOS ML model on the ML trace data. The script automatically saves the learned weights/biases to /home/cc/LinnOSWriterReplayer/mlData directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run-time is around 20 minutes.\n",
    "# Custom loss modifier can be changed by modifiying the custom_loss parameter in the pred1.py script.\n",
    "# Given limited time, we have not tested the inflection point\n",
    "# on Chameleon and currently have applied a constant p85 threshold.\n",
    "for i in 0 1 2 \n",
    "do\n",
    "   ssh cc@\"$ConIP\" python3 LinnOSWriterReplayer/py/pred1.py \\\n",
    "   LinnOSWriterReplayer/mlData/\"mldrive${i}.csv\" > \"mldrive${i}results\".txt\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LinnOS requires each ML trace data to have a seperate directory with its own learned weight/bias files. Hence here we are grouping weight files for each Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" << EOF\n",
    " cd LinnOSWriterReplayer/mlData\n",
    " mkdir -p drive0weights\n",
    " mkdir -p drive1weights\n",
    " mkdir -p drive2weights\n",
    " cp mldrive0.csv.* drive0weights\n",
    " cp mldrive1.csv.* drive1weights\n",
    " cp mldrive2.csv.* drive2weights\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we use the mlHeaderGen.py script to convert saved weight files to LinnOS kernel compatible headers. The output of the mlHeaderGen.py script is configured as the LinnOS kernel source code. The added machine learning header files can be configured (i.e., disabled and enabled) by modifiying the ml_models.h located in /home/cc/linux-5.4.8-linnos/block directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" python3 LinnOSWriterReplayer/mlHeaderGen/mlHeaderGen.py \\\n",
    "Trace sde /home/cc/LinnOSWriterReplayer/mlData/drive0weights /home/cc/linux-5.4.8-linnos/block\n",
    "\n",
    "ssh cc@\"$ConIP\" python3 LinnOSWriterReplayer/mlHeaderGen/mlHeaderGen.py \\\n",
    "Trace sdf /home/cc/LinnOSWriterReplayer/mlData/drive1weights /home/cc/linux-5.4.8-linnos/block\n",
    "\n",
    "ssh cc@\"$ConIP\" python3 LinnOSWriterReplayer/mlHeaderGen/mlHeaderGen.py \\\n",
    "Trace sdg /home/cc/LinnOSWriterReplayer/mlData/drive2weights /home/cc/linux-5.4.8-linnos/block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Install LinnOS kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing the config file\n",
    "# The current kernel config file is copied here for backward compatibility.\n",
    "ssh cc@\"$ConIP\" cp /boot/config-4.15.0-112-generic linux-5.4.8-linnos/.config\n",
    "\n",
    "# Installing the required packages\n",
    "ssh cc@\"$ConIP\" sudo apt-get -y install build-essential libncurses-dev bison flex libssl-dev libelf-dev\n",
    "\n",
    "# Preparing the config file\n",
    "ssh cc@\"$ConIP\" make -C /home/cc/linux-5.4.8-linnos olddefconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compiling the LinnOS Kernel\n",
    "ssh cc@\"$ConIP\" make -C /home/cc/linux-5.4.8-linnos -j $(nproc) > makeLinnosLog.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the LinnOS Kernel\n",
    "ssh cc@\"$ConIP\" sudo make -C /home/cc/linux-5.4.8-linnos modules_install > modulesInstallLinnosLog.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compiling the LinnOS Kernel\n",
    "ssh cc@\"$ConIP\" sudo make -C /home/cc/linux-5.4.8-linnos install  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" sudo update-initramfs -c -k 5.4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" sudo update-grub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh cc@\"$ConIP\" sudo reboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the kernel version to make sure that it is Linux 5.4.8-linnos x86_64\n",
    "ssh cc@\"$ConIP\" uname -mrs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Using the LinnOS, replay the IO traces to obtain ML results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In step-1 we already populated the drive and in step-3 we installed the LinnOS kernel. Hence here we can just run the replayer_fail script to initiate fail over behaviour. More specifically, the fail over behaviour is determined by the LinnOS machine learning model (i.e., If predicted IO request is costly (i.e., high predicted latency), then redirect it to another drive). Note that here we run the replayer on testdrive just like what we did to obtain the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This process takes 5 minutes. If the cell sucessfully runs, you will see \"All done!\" message.\n",
    "ssh cc@\"$ConIP\" << EOF\n",
    " cd LinnOSWriterReplayer\n",
    " sudo ./replayer_fail /dev/sde-/dev/sdf-/dev/sdg \\\n",
    " 'testTraces/testdrive0.'$TraceTag \\\n",
    " 'testTraces/testdrive1.'$TraceTag \\\n",
    " 'testTraces/testdrive2.'$TraceTag py/MLOutput\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the results to graph-friendly format\n",
    "ssh cc@\"$ConIP\" python3 LinnOSWriterReplayer/py/percentile.py 2 read \\\n",
    "LinnOSWriterReplayer/py/MLOutput LinnOSWriterReplayer/py/MLData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp cc@\"$ConIP\":/home/cc/LinnOSWriterReplayer/py/MLDataread_percentile.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To obtain baseline vs LinnOS graph, run the LinnOSResultsPlot.ipynb. The closer the line gets to the upper left, the better its performance is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you encounter with \"Failed to validate token (HTTP 404)\" error,\n",
    "# you can stop/restart your server in https://jupyter.chameleoncloud.org/hub/home to fix it.\n",
    "openstack stack delete \"$RESOURCE_NAME\" --yes --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blazar lease-delete \"$RESOURCE_NAME\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
