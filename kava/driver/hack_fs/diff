diff --color --recursive linux_ecryptfs/crypto.c ecryptfs/crypto.c
517a518,720
>  * ecryptfs_encrypt_pages
>  * @pgs: Pages mapped from the eCryptfs inode for the file; contains
> *        decrypted content that needs to be encrypted (to a temporary
> *        page; not in place) and written out to the lower file
>  *
>  * Encrypt eCryptfs pages.
>  *
>  * Returns zero on success; negative on error
>  */
> 
> int ecryptfs_encrypt_pages(struct page **pgs, unsigned int nr_pages)
> {
> 	struct inode *ecryptfs_inode;
> 	struct ecryptfs_crypt_stat *crypt_stat;
> 	struct page *enc_extent_page = NULL, *src_extent_page = NULL;
> 	int rc = 0;
> 	struct scatterlist *sgs = NULL, *sgd = NULL;
> 	unsigned int i = 0;
> 	u32 sz = 0;
> 	char extent_iv[ECRYPTFS_MAX_IV_BYTES];
> 
> 	if (!nr_pages || !pgs || !pgs[0]) {
> 		goto out;
> 	}
> 
> 	sgs = (struct scatterlist *)kmalloc(
> 		nr_pages * sizeof(struct scatterlist), GFP_KERNEL);
> 	if (!sgs) {
> 		rc = -ENOMEM;
> 		ecryptfs_printk(KERN_ERR, "[kava] Error allocating memory for "
>                 "source scatter list\n");
> 		goto out;
>     }
> 
>     sgd = (struct scatterlist *)kmalloc(
> 		nr_pages * sizeof(struct scatterlist), GFP_KERNEL);
> 	if (!sgd) {
> 		rc = -ENOMEM;
> 		ecryptfs_printk(KERN_ERR, "[kava] Error allocating memory for "
>                 "destination scatter list\n");
> 		kfree(sgs);
> 		goto out;
>     }
> 
>     sg_init_table(sgs, nr_pages);
>     sg_init_table(sgd, nr_pages);
> 
> 	ecryptfs_inode = pgs[0]->mapping->host;
> 	crypt_stat =
>         &(ecryptfs_inode_to_private(ecryptfs_inode)->crypt_stat);
>     BUG_ON(!(crypt_stat->flags & ECRYPTFS_ENCRYPTED));
> 
> 	for (i = 0; i < nr_pages; i++) {
> 		enc_extent_page = alloc_page(GFP_USER);
> 		if (!enc_extent_page) {
> 			rc = -ENOMEM;
>             ecryptfs_printk(KERN_ERR, "[kava] Error allocating memory for "
>                     "encrypted extent\n");
> 			for (sz = 0; sz < i; sz++) {
> 				enc_extent_page = sg_page(sgd + sz);
> 				__free_page(enc_extent_page);
> 			}
> 			goto higher_out;
> 		}
> 
> 		sg_set_page(sgs + i, pgs[i], PAGE_SIZE, 0);
> 		sg_set_page(sgd + i, enc_extent_page, PAGE_SIZE, 0);
> 	}
> 
> 	rc = crypt_scatterlist(crypt_stat, sgd, sgs, PAGE_SIZE * nr_pages,
>             extent_iv, ENCRYPT);
>     if (rc) {
>         printk(KERN_ERR "%s: Error encrypting extents in scatter list; "
>                 "rc = [%d]\n", __func__, rc);
> 	    for (i = 0; i < nr_pages; i++) {
>             __free_page(sg_page(sgd + i));
>         }
>         goto higher_out;
>     }
> 
> 	for (i = 0; i < nr_pages; i++) {
> 		int ret;
> 		enc_extent_page = sg_page(sgd + i);
> 		ret = ecryptfs_write_lower_page_segment(ecryptfs_inode,
>                 enc_extent_page, 0, PAGE_SIZE);
>         __free_page(enc_extent_page);
> 
>         src_extent_page = sg_page(sgs + i);
> 		if (ret < 0) {
> 			ecryptfs_printk(KERN_ERR, "Error attempting "
> 					"to write lower page; rc = [%d]\n", ret);
> 			ClearPageUptodate(src_extent_page);
> 			rc = ret;
> 		} else {
> 			SetPageUptodate(src_extent_page);
> 			if (PageLocked(src_extent_page))
> 				unlock_page(src_extent_page);
> 		}
> 	}
> 
> higher_out:
> 	kfree(sgs);
> 	kfree(sgd);
> out:
> 	return rc;
> }
> 
> int ecryptfs_encrypt_pages2(struct page **pgs, unsigned int nr_pages)
> {
> 	struct inode *ecryptfs_inode;
> 	struct ecryptfs_crypt_stat *crypt_stat;
> 	loff_t lower_offset;
> 	struct page *enc_extent_page  = NULL;
> 	char *enc_extent_virt;
> 	int rc = 0;
> 
> 	struct scatterlist *sgs = NULL, *sgd = NULL;
> 	unsigned int i = 0;
> 	u32 sz = 0;
> 	char extent_iv[ECRYPTFS_MAX_IV_BYTES];
> 
> 	if (!nr_pages || !pgs || !pgs[0]) {
> 		goto out;
> 	}
> 
> 	sgs = (struct scatterlist *)kmalloc(
> 		nr_pages * sizeof(struct scatterlist), GFP_KERNEL);
> 	if (!sgs) {
> 		rc = -ENOMEM;
> 		ecryptfs_printk(KERN_ERR, "[kava] Error allocating memory for "
>                 "source scatter list\n");
> 		goto out;
>     }
> 
>     sgd = (struct scatterlist *)kmalloc(
> 		nr_pages * sizeof(struct scatterlist), GFP_KERNEL);
> 	if (!sgd) {
> 		ecryptfs_printk(KERN_ERR, "[kava] Error allocating memory for "
>                 "destination scatter list\n");
> 		rc = -ENOMEM;
> 		kfree(sgs);
> 		goto out;
>    }
> 
>     sg_init_table(sgs, nr_pages);
>     sg_init_table(sgd, nr_pages);
> 
>     ecryptfs_inode = pgs[0]->mapping->host;
> 	crypt_stat = &(ecryptfs_inode_to_private(ecryptfs_inode)->crypt_stat);
> 
> 	for (i = 0; i < nr_pages; i++) {
> 		enc_extent_page = alloc_page(GFP_USER);
> 		if (!enc_extent_page) {
> 			rc = -ENOMEM;
>             ecryptfs_printk(KERN_ERR, "[kava] Error allocating memory for "
>                     "encrypted extent\n");
> 			for (sz = 0; sz < i; sz++) {
> 				__free_page(sg_page(sgd + sz));
> 			}
> 			goto higher_out;
> 		}
> 
> 		sg_set_page(sgs + i, pgs[i], PAGE_SIZE, 0);
> 		sg_set_page(sgd + i, enc_extent_page, PAGE_SIZE, 0);
> 	}
> 
> 	rc = crypt_scatterlist(crypt_stat, sgd, sgs, PAGE_SIZE * nr_pages,
>             extent_iv, ENCRYPT);
>     if (rc) {
>         printk(KERN_ERR "%s: Error encrypting extents in scatter list; "
>                 "rc = [%d]\n", __func__, rc);
> 	    for (i = 0; i < nr_pages; i++) {
>             __free_page(sg_page(sgd + i));
>         }
>         goto higher_out;
>     }
> 
> 	for (i = 0; i < nr_pages; i++) {
>         int ret;
> 
> 		enc_extent_page = sg_page(sgd + i);
> 		enc_extent_virt = kmap(enc_extent_page);
>         lower_offset = lower_offset_for_page(crypt_stat, pgs[i]);
> 		ret = ecryptfs_write_lower(ecryptfs_inode, enc_extent_virt, lower_offset,
>                 PAGE_SIZE);
> 		kunmap(enc_extent_page);
> 		__free_page(enc_extent_page);
> 
> 		if (ret < 0) {
> 			ecryptfs_printk(KERN_ERR, "Error attempting "
> 					"to write lower page; rc = [%d]\n", ret);
> 			rc = ret;
> 		}
> 	}
> 
> higher_out:
> 	kfree(sgs);
> 	kfree(sgd);
> out:
> 	return rc;
> }
> 
> /**
573a777,847
> /**
>  * ecryptfs_decrypt_pages
>  * @pgs: Pages mapped from the eCryptfs inode for the file; data read
>  *       and decrypted from the lower file will be written into this
>  *       page
>  * @nr_pages: Number of pages to be decrypted
>  *
>  * Returns zero on success; negative on error
>  */
> int ecryptfs_decrypt_pages(struct page **pgs, unsigned int nr_pages)
> {
>     struct inode *ecryptfs_inode;
>     struct ecryptfs_crypt_stat *crypt_stat;
>     char extent_iv[ECRYPTFS_MAX_IV_BYTES];
>     struct scatterlist *sgs = NULL;
>     int rc = 0;
>     unsigned int i = 0;
> 
>     if (!nr_pages || !pgs || !pgs[0]) {
>         goto out;
>     }
> 
>     sgs = (struct scatterlist *)kmalloc(nr_pages * sizeof(struct scatterlist),
>             GFP_KERNEL);
>     if (!sgs) {
>         rc = -EFAULT;
>         ecryptfs_printk(KERN_ERR, "[kava] Error allocating memory for "
>                 "source scatter list\n");
>         goto out;
>     }
> 
>     sg_init_table(sgs, nr_pages);
> 
>     ecryptfs_inode = pgs[0]->mapping->host;
>     crypt_stat =
>         &(ecryptfs_inode_to_private(ecryptfs_inode)->crypt_stat);
>     BUG_ON(!(crypt_stat->flags & ECRYPTFS_ENCRYPTED));
> 
>     for (i = 0; i < nr_pages; i++) {
>     	char *page_virt;
>     	loff_t lower_offset;
>     	lower_offset = lower_offset_for_page(crypt_stat, pgs[i]);
>     	page_virt = kmap(pgs[i]);
> 
>     	rc = ecryptfs_read_lower(page_virt, lower_offset, PAGE_SIZE,
>                 ecryptfs_inode);
>         if (rc < 0) {
>             ecryptfs_printk(KERN_ERR, "Error attempting to read lower page; "
>                     "rc = [%d] \n", rc);
>         }
> 
>         kunmap(pgs[i]);
>         flush_dcache_page(pgs[i]);
>         sg_set_page(sgs + i, pgs[i], PAGE_SIZE, 0);
>     }
> 
>     rc = crypt_scatterlist(crypt_stat, sgs, sgs, nr_pages * PAGE_SIZE,
>             extent_iv, DECRYPT);
> 
>     for (i = 0; i < nr_pages; i++) {
>         SetPageUptodate(pgs[i]);
>         if (PageLocked(pgs[i]))
>             unlock_page(pgs[i]);
> 	}
> 
> out:
>     if (sgs)
>         kfree(sgs);
>     return (rc >= 0 ? 0 : rc);
> }
> 
601c875
< 						    crypt_stat->cipher, "ecb");
---
> 						    crypt_stat->cipher, "kava_ecb");
1021,1024c1295,1296
< 	if (rc < 0)
< 		return rc;
< 	else if (rc < ECRYPTFS_SIZE_AND_MARKER_BYTES)
< 		return -EINVAL;
---
> 	if (rc < ECRYPTFS_SIZE_AND_MARKER_BYTES)
> 		return rc >= 0 ? -EINVAL : rc;
1386,1389c1658,1659
< 	if (rc < 0)
< 		return rc;
< 	else if (rc < ECRYPTFS_SIZE_AND_MARKER_BYTES)
< 		return -EINVAL;
---
> 	if (rc < ECRYPTFS_SIZE_AND_MARKER_BYTES)
> 		return rc >= 0 ? -EINVAL : rc;
Only in ecryptfs: ecryptfs_kernel.h
Only in linux_ecryptfs: ecryptfs.mod.c
diff --color --recursive linux_ecryptfs/file.c ecryptfs/file.c
36a37,64
>  * ecryptfs_file_read_iter - ecryptfs filesystem read routine
>  * @iocb:	kernel I/O control block
>  * @iter:	destination for the data read
>  *
>  * This is the derivative of "read_iter()" routine for all filesystems
>  * that can use the page cache directly.
>  */
> static ssize_t
> ecryptfs_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
> {
>     size_t count = iov_iter_count(iter);
> 	ssize_t retval = 0;
> 
>     if (!count) {
>         goto out; /* skip atime */
>     }
> 
> 	if (iocb->ki_flags & IOCB_DIRECT) {
>         pr_err("IOCB not supported\n");
>         goto out;
> 	}
> 
>     retval = ecryptfs_file_buffered_read2(iocb, iter, retval);
> out:
> 	return retval;
> }
> 
> /**
53c81
< 	rc = generic_file_read_iter(iocb, to);
---
> 	rc = ecryptfs_file_read_iter(iocb, to);
408a437,454
> static ssize_t ecryptfs_file_read(struct file *file, char __user *buf,
>                     size_t size, loff_t *poffset)
> {
>     ssize_t rc;
>     rc = ecryptfs_read(file_inode(file), (char *)buf, *poffset, size);
>     if (rc >= 0)
>         *poffset += size;
>     return rc;
> }
> 
> static ssize_t ecryptfs_file_write(struct file *file, const char __user *data,
> 				   size_t size, loff_t *poffset)
> {
>     ecryptfs_write2(file_inode(file), (char *)data, *poffset, size);
>     *poffset += size;
>     return size;
> }
> 
431a478,479
>     //.read = ecryptfs_file_read,
> 	.write = ecryptfs_file_write,
Only in ecryptfs: .gitignore
diff --color --recursive linux_ecryptfs/inode.c ecryptfs/inode.c
328,329c328
< 	struct path *path = ecryptfs_dentry_to_lower_path(dentry->d_parent);
< 	struct inode *inode, *lower_inode;
---
> 	struct inode *inode, *lower_inode = d_inode(lower_dentry);
330a330
> 	struct vfsmount *lower_mnt;
338a339
> 	lower_mnt = mntget(ecryptfs_dentry_to_lower_mnt(dentry->d_parent));
340c341
< 				d_inode(path->dentry));
---
> 				d_inode(lower_dentry->d_parent));
344c345
< 	dentry_info->lower_path.mnt = mntget(path->mnt);
---
> 	dentry_info->lower_path.mnt = lower_mnt;
347,355c348
< 	/*
< 	 * negative dentry can go positive under us here - its parent is not
< 	 * locked.  That's OK and that could happen just as we return from
< 	 * ecryptfs_lookup() anyway.  Just need to be careful and fetch
< 	 * ->d_inode only once - it's not stable here.
< 	 */
< 	lower_inode = READ_ONCE(lower_dentry->d_inode);
< 
< 	if (!lower_inode) {
---
> 	if (d_really_is_negative(lower_dentry)) {
Only in ecryptfs: Kconfig
diff --color --recursive linux_ecryptfs/keystore.c ecryptfs/keystore.c
1321c1321
< 		goto out_free;
---
> 		goto out;
Only in ecryptfs: load.sh
diff --color --recursive linux_ecryptfs/main.c ecryptfs/main.c
509,514d508
< 	if (!dev_name) {
< 		rc = -EINVAL;
< 		err = "Device name cannot be null";
< 		goto out;
< 	}
< 
Only in ecryptfs: Makefile
diff --color --recursive linux_ecryptfs/messaging.c ecryptfs/messaging.c
395d394
< 		kfree(ecryptfs_daemon_hash);
diff --color --recursive linux_ecryptfs/mmap.c ecryptfs/mmap.c
30a31,32
> #include <linux/pagemap.h>
> #include <linux/pagevec.h>
81a84,249
> /**
>  * ecryptfs_writepage
>  * @mapping: Page that is locked before this call is made
>  *
>  * Returns zero on success; non-zero otherwise
>  *
>  * This was inspired by write_cache_pages from /mm/page-writeback.c
>  */
> static int ecryptfs_writepages(struct address_space *mapping,
> 			       struct writeback_control *wbc)
> {
> 	int ret = 0;
> 	int done = 0;
> 	struct pagevec pvec;
> 	int nr_pages;
> 	pgoff_t uninitialized_var(writeback_index);
> 	pgoff_t index;
> 	pgoff_t end;		/* Inclusive */
> 	pgoff_t done_index;
> 	int cycled;
> 	int range_whole = 0;
> 	int tag;
> 	struct page **pgs;
> 	int pg_idx;
> 
> 	pgs = kmalloc(sizeof(struct page *) * PAGEVEC_SIZE, GFP_KERNEL);
> 	if (!pgs) {
> 		printk("[g-ecryptfs] Error: pgs alloc failed!\n");
> 		return -EFAULT;
> 	}
> 
> 	pagevec_init(&pvec);
> 	if (wbc->range_cyclic) {
> 		writeback_index = mapping->writeback_index; /* prev offset */
> 		index = writeback_index;
> 		if (index == 0)
> 			cycled = 1;
> 		else
> 			cycled = 0;
> 		end = -1;
> 	} else {
> 		index = wbc->range_start >> PAGE_SHIFT;
> 		end = wbc->range_end >> PAGE_SHIFT;
> 		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
> 			range_whole = 1;
> 		cycled = 1; /* ignore range_cyclic tests */
> 	}
> 	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
> 		tag = PAGECACHE_TAG_TOWRITE;
> 	else
> 		tag = PAGECACHE_TAG_DIRTY;
> retry:
> 	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
> 		tag_pages_for_writeback(mapping, index, end);
> 	done_index = index;
> 	while (!done && (index <= end)) {
> 		int i;
> 
> 		nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,
>                 tag);
> 		if (nr_pages == 0)
> 			break;
> 
> 		pg_idx = 0;
> 
> 		for (i = 0; i < nr_pages; i++) {
> 		    struct page *page = pvec.pages[i];
> 
>             done_index = page->index;
> 
>             lock_page(page);
> 
> 			/*
> 			 * Page truncated or invalidated. We can freely skip it
> 			 * then, even for data integrity operations: the page
> 			 * has disappeared concurrently, so there could be no
> 			 * real expectation of this data interity operation
> 			 * even if there is now a new, dirty page at the same
> 			 * pagecache address.
> 			 */
> 			if (unlikely(page->mapping != mapping)) {
> continue_unlock:
> 				unlock_page(page);
> 				continue;
> 			}
> 
> 			if (!PageDirty(page)) {
> 				/* someone wrote it for us */
> 				goto continue_unlock;
> 			}
> 
> 			if (PageWriteback(page)) {
> 				if (wbc->sync_mode != WB_SYNC_NONE)
> 					wait_on_page_writeback(page);
> 				else
> 					goto continue_unlock;
> 			}
> 
> 			BUG_ON(PageWriteback(page));
> 			if (!clear_page_dirty_for_io(page))
> 				goto continue_unlock;
> 
> 			pgs[pg_idx++] = page;
> 		}
> 
> 		ret = ecryptfs_encrypt_pages(pgs, pg_idx);
> 		mapping_set_error(mapping, ret);
> 
> 		for (i = 0; i < nr_pages; i++) {
>             struct page *page = pvec.pages[i];
> 
> 			if (unlikely(ret)) {
> 				if (ret == AOP_WRITEPAGE_ACTIVATE) {
> 					if (PageLocked(page))
> 						unlock_page(page);
> 					ret = 0;
> 				} else {
> 					/*
> 					 * done_index is set past this page,
> 					 * so media errors will not choke
> 					 * background writeout for the entire
> 					 * file. This has consequences for
> 					 * range_cyclic semantics (ie. it may
> 					 * not be suitable for data integrity
> 					 * writeout).
> 					 */
>                     done_index = page->index + 1;
> 					done = 1;
> 					break;
> 				}
> 			}
> 
> 			/*
> 			 * We stop writing back only if we are not doing
> 			 * integrity sync. In case of integrity sync we have to
> 			 * keep going until we have written all the pages
> 			 * we tagged for writeback prior to entering this loop.
> 			 */
> 			if (--wbc->nr_to_write <= 0 &&
> 			    wbc->sync_mode == WB_SYNC_NONE) {
> 				done = 1;
> 				break;
> 			}
> 		}
> 		pagevec_release(&pvec);
> 		cond_resched();
> 	}
> 	if (!cycled && !done) {
> 		/*
> 		 * range_cyclic:
> 		 * We hit the last page and there is more work to be done: wrap
> 		 * back to the start of the file
> 		 */
> 		cycled = 1;
> 		index = 0;
> 		end = writeback_index - 1;
> 		goto retry;
> 	}
> 	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
> 		mapping->writeback_index = done_index;
> 
> 	kfree(pgs);
> 
> 	return ret;
> }
> 
246a415,484
>  * ecryptfs_readpages
>  * @file: An eCryptfs file
>  * @page: Page from eCryptfs inode mapping into which to stick the read data
>  *
>  * Read in multiple pages, decrypting if necessary.
>  *
>  * Returns zero on success; non-zero on error.
>  */
> static int ecryptfs_readpages(struct file *filp, struct address_space *mapping,
> 			      struct list_head *pages, unsigned nr_pages)
> {
> 	struct ecryptfs_crypt_stat *crypt_stat =
> 	    		&ecryptfs_inode_to_private(mapping->host)->crypt_stat;
> 	struct page **pgs = NULL;
> 	unsigned int page_idx = 0;
> 	int rc = 0;
> 	int nodec = 0;	//no decryption needed flag
> 
> 	if (!crypt_stat
> 	    || !(crypt_stat->flags & ECRYPTFS_ENCRYPTED)
> 	    || (crypt_stat->flags & ECRYPTFS_NEW_FILE)
> 	    || (crypt_stat->flags & ECRYPTFS_VIEW_AS_ENCRYPTED)) {
> 	    nodec = 1;
> 	}
> 
> 	if (!nodec) {
> 	    pgs = (struct page **)kmalloc(nr_pages * sizeof(struct page *), GFP_KERNEL);
> 	    if (!pgs) {
>             return -EFAULT;
> 	    }
> 	}
> 
> 	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
> 	    struct page *page = list_entry(pages->prev, struct page, lru);
> 	    list_del(&page->lru);
> 	    if (add_to_page_cache_lru(page, mapping, page->index, GFP_KERNEL)) {
> 			printk(KERN_INFO "[kava] INFO: cannot add page %lu to cache lru\n",
> 			       (unsigned long)(page->index));
> 	    }
>         else {
>             if (nodec)
>                 rc |= ecryptfs_readpage(filp, page);
>         }
> 
>         if (nodec)
>             put_page(page);
>         else
>             pgs[page_idx] = page;
>     }
> 
>     if (!nodec) {
>         rc = ecryptfs_decrypt_pages(pgs, nr_pages);
> 
>         for (page_idx = 0; page_idx < nr_pages; page_idx++) {
>             if (rc)
>                 ClearPageUptodate(pgs[page_idx]);
>             else
>                 SetPageUptodate(pgs[page_idx]);
>             unlock_page(pgs[page_idx]);
> 
>             put_page(pgs[page_idx]);
>         }
> 
>         kfree(pgs);
>     }
> 
> 	return 0;
> }
> 
> /**
558a797,798
>     .readpages = ecryptfs_readpages,
>     .writepages = ecryptfs_writepages,
diff --color --recursive linux_ecryptfs/read_write.c ecryptfs/read_write.c
25a26,27
> #include <linux/uio.h>
> #include <linux/swap.h>
220a223,390
>  * ecryptfs_write2
>  * @ecryptfs_inode: The eCryptfs file into which to write
>  * @data: Virtual address where data to write is located
>  * @offset: Offset in the eCryptfs file at which to begin writing the
>  *          data from @data
>  * @size: The number of bytes to write from @data
>  *
>  * Write an arbitrary amount of data to an arbitrary location in the
>  * eCryptfs inode page cache. This is done on a page-by-page, and then
>  * by an extent-by-extent, basis; individual extents are encrypted and
>  * written to the lower page cache (via VFS writes). This function
>  * takes care of all the address translation to locations in the lower
>  * filesystem; it also handles truncate events, writing out zeros
>  * where necessary.
>  *
>  * Returns zero on success; non-zero otherwise
>  */
> int ecryptfs_write2(struct inode *ecryptfs_inode, char __user *data, loff_t offset,
> 		   size_t size)
> {
> 	struct address_space *mapping = ecryptfs_inode->i_mapping;
> 	struct page *ecryptfs_page;
> 	struct ecryptfs_crypt_stat *crypt_stat;
> 	char *ecryptfs_page_virt;
> 	loff_t ecryptfs_file_size = i_size_read(ecryptfs_inode);
> 	loff_t data_offset = offset;
> 	loff_t pos;
> 	int rc = 0;
> 	struct page **pgs;
> 	int nr_pgs = DIV_ROUND_UP(size, PAGE_SIZE);
> 	int i = 0;
> 
> 	pgs = kmalloc(nr_pgs * sizeof(struct page *), GFP_KERNEL);
> 	if (!pgs) {
> 	    rc = -ENOMEM;
> 	    printk(KERN_ERR "[kava] Error allocating pages\n");
> 	    goto out;
> 	}
> 
> 	crypt_stat = &ecryptfs_inode_to_private(ecryptfs_inode)->crypt_stat;
> 	/*
> 	 * if we are writing beyond current size, then start pos
> 	 * at the current size - we'll fill in zeros from there.
> 	 */
> 	if (offset > ecryptfs_file_size)
> 		pos = ecryptfs_file_size;
> 	else
> 		pos = offset;
> 
> 	while (pos < (offset + size)) {
> 		pgoff_t ecryptfs_page_idx = (pos >> PAGE_SHIFT);
> 		size_t start_offset_in_page = (pos & ~PAGE_MASK);
> 		size_t num_bytes = (PAGE_SIZE - start_offset_in_page);
> 		loff_t total_remaining_bytes = ((offset + size) - pos);
> 
>         if (fatal_signal_pending(current)) {
>             rc = -EINTR;
>             break;
>         }
> 
> 		if (num_bytes > total_remaining_bytes)
> 			num_bytes = total_remaining_bytes;
> 		if (pos < offset) {
> 			/* remaining zeros to write, up to destination offset */
> 			loff_t total_remaining_zeros = (offset - pos);
> 
> 			if (num_bytes > total_remaining_zeros)
> 				num_bytes = total_remaining_zeros;
> 		}
> 		//ecryptfs_page = ecryptfs_get_locked_page(ecryptfs_inode,
> 		//					 ecryptfs_page_idx);
>         /* The following change is only correct when overwriting the whole page.
>          * TODO: use ecryptfs_get_locked_page when only modify part of the page.
>          */
> 		ecryptfs_page = page_cache_alloc(mapping);
> 		if (IS_ERR(ecryptfs_page)) {
> 			rc = PTR_ERR(ecryptfs_page);
> 			printk(KERN_ERR "%s: Error getting page at "
> 			       "index [%ld] from eCryptfs inode "
> 			       "mapping; rc = [%d]\n", __func__,
> 			       ecryptfs_page_idx, rc);
> 			goto out;
> 		}
> 		rc = add_to_page_cache_lru(ecryptfs_page, mapping, ecryptfs_page_idx,
> 				mapping_gfp_constraint(mapping, GFP_KERNEL));
> 		if (rc) {
> 			put_page(ecryptfs_page);
> 			printk(KERN_ERR "%s: Error adding page to cache lru at "
> 			       "index [%ld] from eCryptfs inode "
> 			       "mapping; rc = [%d]\n", __func__,
> 			       ecryptfs_page_idx, rc);
> 			goto out;
> 		}
>         ClearPageError(ecryptfs_page);
> 
> 		ecryptfs_page_virt = kmap(ecryptfs_page);
> 
> 		/*
> 		 * pos: where we're now writing, offset: where the request was
> 		 * If current pos is before request, we are filling zeros
> 		 * If we are at or beyond request, we are writing the *data*
> 		 * If we're in a fresh page beyond eof, zero it in either case
> 		 */
> 		if (pos < offset || !start_offset_in_page) {
> 			/* We are extending past the previous end of the file.
> 			 * Fill in zero values to the end of the page */
> 			memset(((char *)ecryptfs_page_virt
> 				+ start_offset_in_page), 0,
> 				PAGE_SIZE - start_offset_in_page);
> 		}
> 
> 		/* pos >= offset, we are now writing the data request */
> 		if (pos >= offset) {
> 			copy_from_user(((char *)ecryptfs_page_virt
>                 + start_offset_in_page),
> 			       (data + data_offset), num_bytes);
> 			data_offset += num_bytes;
> 		}
> 		kunmap(ecryptfs_page);
> 		flush_dcache_page(ecryptfs_page);
> 		SetPageUptodate(ecryptfs_page);
> 		unlock_page(ecryptfs_page);
> 		if (crypt_stat->flags & ECRYPTFS_ENCRYPTED) {
> 		    pgs[i++] = ecryptfs_page;
> 		}
> 		else {
> 		    rc = ecryptfs_write_lower_page_segment(ecryptfs_inode,
> 						ecryptfs_page,
> 						start_offset_in_page,
> 						data_offset);
> 		    put_page(ecryptfs_page);
> 		    if (rc) {
>                 printk(KERN_ERR "%s: Error encrypting "
>                        "page; rc = [%d]\n", __func__, rc);
>                 goto out;
> 		    }
> 		}
> 		pos += num_bytes;
> 	}
> 
> 	if (crypt_stat->flags & ECRYPTFS_ENCRYPTED) {
> 	    rc = ecryptfs_encrypt_pages2(pgs, nr_pgs);
> 	    for (i = 0; i < nr_pgs; i++)
> 		    put_page(pgs[i]);
> 	    kfree(pgs);
> 	}
> 
> 	if (pos > ecryptfs_file_size) {
> 		i_size_write(ecryptfs_inode, (offset + size));
> 		if (crypt_stat->flags & ECRYPTFS_ENCRYPTED) {
>             int rc2;
> 
> 			rc2 = ecryptfs_write_inode_size_to_metadata(ecryptfs_inode);
> 			if (rc2) {
> 				printk(KERN_ERR	"Problem with "
> 				       "ecryptfs_write_inode_size_to_metadata; "
> 				       "rc = [%d]\n", rc);
>                 if (!rc)
>                     rc = rc2;
> 				goto out;
> 			}
> 		}
> 	}
> out:
> 	return rc;
> }
> 
> /**
274a445,1094
> }
> 
> /**
>  * ecryptfs_read
>  * @ecryptfs_inode: The eCryptfs file from which to read
>  * @buf: The virtual address into which to write the data read (and
>  *        possibly decrypted) from the lower file
>  * @offset: The offset in the decrypted view of the file from which to
>  *          read into @data
>  * @size: The number of bytes to read into @buf
>  *
>  * Read an arbitrary amount of data from an arbitrary location in the
>  * eCryptfs page cache. This is done on an extent-by-extent basis;
>  * individual extents are decrypted and read from the lower page
>  * cache (via VFS reads). This function takes care of all the
>  * address translation to locations in the lower filesystem.
>  *
>  * Returns non-negative on success; negative otherwise
>  */
> ssize_t ecryptfs_read(struct inode *ecryptfs_inode, char __user *buf, loff_t offset,
>             size_t size)
> {
> 	struct page *ecryptfs_page;
>     struct ecryptfs_crypt_stat *crypt_stat;
> 	char *ecryptfs_page_virt;
> 	loff_t ecryptfs_file_size = i_size_read(ecryptfs_inode);
> 	loff_t data_offset = offset;
> 	loff_t pos;
> 	int rc = 0, chk = 0;
> 
>     crypt_stat = &ecryptfs_inode_to_private(ecryptfs_inode)->crypt_stat;
> 
> 	if (offset >= ecryptfs_file_size || size == 0) {
> 		goto out;
> 	}
> 
>     if (offset + size > ecryptfs_file_size) {
>         size = ecryptfs_file_size - offset;
>     }
> 
> 	pos = offset;
> 	while (pos < (offset + size)) {
> 		pgoff_t ecryptfs_page_idx = (pos >> PAGE_SHIFT);
> 		size_t start_offset_in_page = (pos & ~PAGE_MASK);
> 		size_t num_bytes = (PAGE_SIZE - start_offset_in_page);
> 		size_t total_remaining_bytes = ((offset + size) - pos);
> 
> 		if (num_bytes > total_remaining_bytes)
>             num_bytes = total_remaining_bytes;
> 
> 		ecryptfs_page = ecryptfs_get_locked_page(ecryptfs_inode, ecryptfs_page_idx);
> 		if (IS_ERR(ecryptfs_page)) {
> 			rc = PTR_ERR(ecryptfs_page);
> 			printk(KERN_ERR "%s: Error getting page at "
> 			       "index [%ld] from eCryptfs inode "
> 			       "mapping; rc = [%d]\n", __func__,
> 			       ecryptfs_page_idx, rc);
> 			goto out;
> 		}
> 
>         if (crypt_stat->flags & ECRYPTFS_ENCRYPTED) {
>             rc = ecryptfs_decrypt_page(ecryptfs_page);
>         }
>         else {
>             rc = ecryptfs_read_lower_page_segment(ecryptfs_page, ecryptfs_page_idx,
>                     start_offset_in_page, num_bytes, ecryptfs_inode);
>         }
>         if (rc) {
>             printk(KERN_ERR "%s: Error decrypting page; "
>                     "rc = [%d]\n", __func__, rc);
>             ClearPageUptodate(ecryptfs_page);
>         }
>         else {
> 		    SetPageUptodate(ecryptfs_page);
>         }
> 
> 		ecryptfs_page_virt = kmap(ecryptfs_page);
> 		chk = copy_to_user(buf + data_offset,
> 		       (char *)ecryptfs_page_virt + start_offset_in_page,
> 		       num_bytes);
> 		kunmap(ecryptfs_page);
> 		unlock_page(ecryptfs_page);
> 		//put_page(ecryptfs_page);
> 
>         if (chk > 0) {
>             rc = data_offset - offset + num_bytes - chk;
>             goto out;
>         }
> 
> 		pos += num_bytes;
> 		data_offset += num_bytes;
> 	}
> 
>     rc = size;
> 
> out:
>     return rc;
> }
> 
> /*
>  * CD/DVDs are error prone. When a medium error occurs, the driver may fail
>  * a _large_ part of the i/o request. Imagine the worst scenario:
>  *
>  *      ---R__________________________________________B__________
>  *         ^ reading here                             ^ bad block(assume 4k)
>  *
>  * read(R) => miss => readahead(R...B) => media error => frustrating retries
>  * => failing the whole request => read(R) => read(R+1) =>
>  * readahead(R+1...B+1) => bang => read(R+2) => read(R+3) =>
>  * readahead(R+3...B+2) => bang => read(R+3) => read(R+4) =>
>  * readahead(R+4...B+3) => bang => read(R+4) => read(R+5) => ......
>  *
>  * It is going insane. Fix it by quickly scaling down the readahead size.
>  */
> static void shrink_readahead_size_eio(struct file *filp,
> 					struct file_ra_state *ra)
> {
> 	ra->ra_pages /= 4;
> }
> 
> /**
>  * ecryptfs_file_buffered_read - ecryptfs file read routine
>  * @iocb:	the iocb to read
>  * @iter:	data destination
>  * @written:	already copied
>  *
>  * This is a generic file read routine, and uses the
>  * mapping->a_ops->readpage() function for the actual low-level stuff.
>  *
>  * This is really ugly. But the goto's actually try to clarify some
>  * of the logic when it comes to error handling etc.
>  */
> ssize_t ecryptfs_file_buffered_read(struct kiocb *iocb,
> 		struct iov_iter *iter, ssize_t written)
> {
> 	struct file *filp = iocb->ki_filp;
> 	struct address_space *mapping = filp->f_mapping;
> 	struct inode *inode = mapping->host;
> 	struct file_ra_state *ra = &filp->f_ra;
> 	loff_t *ppos = &iocb->ki_pos;
> 	pgoff_t index;
> 	pgoff_t last_index;
> 	pgoff_t prev_index;
> 	unsigned long offset;      /* offset into pagecache page */
> 	unsigned int prev_offset;
> 	int error = 0;
> 
> 	if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
> 		return 0;
> 	iov_iter_truncate(iter, inode->i_sb->s_maxbytes);
> 
> 	index = *ppos >> PAGE_SHIFT;
> 	prev_index = ra->prev_pos >> PAGE_SHIFT;
> 	prev_offset = ra->prev_pos & (PAGE_SIZE-1);
> 	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
> 	offset = *ppos & ~PAGE_MASK;
> 
> 	for (;;) {
> 		struct page *page;
> 		pgoff_t end_index;
> 		loff_t isize;
> 		unsigned long nr, ret;
> 
> 		cond_resched();
> find_page:
> 		if (fatal_signal_pending(current)) {
> 			error = -EINTR;
> 			goto out;
> 		}
> 
> 		page = find_get_page(mapping, index);
> 		if (!page) {
> 			if (iocb->ki_flags & IOCB_NOWAIT)
> 				goto would_block;
> 			page_cache_sync_readahead(mapping,
> 					ra, filp,
> 					index, last_index - index);
> 			page = find_get_page(mapping, index);
> 			if (unlikely(page == NULL))
> 				goto no_cached_page;
> 		}
> 		if (PageReadahead(page)) {
> 			page_cache_async_readahead(mapping,
> 					ra, filp, page,
> 					index, last_index - index);
> 		}
> 		if (!PageUptodate(page)) {
> 			if (iocb->ki_flags & IOCB_NOWAIT) {
> 				put_page(page);
> 				goto would_block;
> 			}
> 
> 			/*
> 			 * See comment in do_read_cache_page on why
> 			 * wait_on_page_locked is used to avoid unnecessarily
> 			 * serialisations and why it's safe.
> 			 */
> 			error = wait_on_page_locked_killable(page);
> 			if (unlikely(error))
> 				goto readpage_error;
> 			if (PageUptodate(page))
> 				goto page_ok;
> 
> 			if (inode->i_blkbits == PAGE_SHIFT ||
> 					!mapping->a_ops->is_partially_uptodate)
> 				goto page_not_up_to_date;
> 			/* pipes can't handle partially uptodate pages */
> 			if (unlikely(iter->type & ITER_PIPE))
> 				goto page_not_up_to_date;
> 			if (!trylock_page(page))
> 				goto page_not_up_to_date;
> 			/* Did it get truncated before we got the lock? */
> 			if (!page->mapping)
> 				goto page_not_up_to_date_locked;
> 			if (!mapping->a_ops->is_partially_uptodate(page,
> 							offset, iter->count))
> 				goto page_not_up_to_date_locked;
> 			unlock_page(page);
> 		}
> page_ok:
> 		/*
> 		 * i_size must be checked after we know the page is Uptodate.
> 		 *
> 		 * Checking i_size after the check allows us to calculate
> 		 * the correct value for "nr", which means the zero-filled
> 		 * part of the page is not copied back to userspace (unless
> 		 * another truncate extends the file - this is desired though).
> 		 */
> 
> 		isize = i_size_read(inode);
> 		end_index = (isize - 1) >> PAGE_SHIFT;
> 		if (unlikely(!isize || index > end_index)) {
> 			put_page(page);
> 			goto out;
> 		}
> 
> 		/* nr is the maximum number of bytes to copy from this page */
> 		nr = PAGE_SIZE;
> 		if (index == end_index) {
> 			nr = ((isize - 1) & ~PAGE_MASK) + 1;
> 			if (nr <= offset) {
> 				put_page(page);
> 				goto out;
> 			}
> 		}
> 		nr = nr - offset;
> 
> 		/* If users can be writing to this page using arbitrary
> 		 * virtual addresses, take care about potential aliasing
> 		 * before reading the page on the kernel side.
> 		 */
> 		if (mapping_writably_mapped(mapping))
> 			flush_dcache_page(page);
> 
> 		/*
> 		 * When a sequential read accesses a page several times,
> 		 * only mark it as accessed the first time.
> 		 */
> 		if (prev_index != index || offset != prev_offset)
> 			mark_page_accessed(page);
> 		prev_index = index;
> 
> 		/*
> 		 * Ok, we have the page, and it's up-to-date, so
> 		 * now we can copy it to user space...
> 		 */
> 
> 		ret = copy_page_to_iter(page, offset, nr, iter);
> 		offset += ret;
> 		index += offset >> PAGE_SHIFT;
> 		offset &= ~PAGE_MASK;
> 		prev_offset = offset;
> 
> 		put_page(page);
> 		written += ret;
> 		if (!iov_iter_count(iter))
> 			goto out;
> 		if (ret < nr) {
> 			error = -EFAULT;
> 			goto out;
> 		}
> 		continue;
> 
> page_not_up_to_date:
> 		/* Get exclusive access to the page ... */
> 		error = lock_page_killable(page);
> 		if (unlikely(error))
> 			goto readpage_error;
> 
> page_not_up_to_date_locked:
> 		/* Did it get truncated before we got the lock? */
> 		if (!page->mapping) {
> 			unlock_page(page);
> 			put_page(page);
> 			continue;
> 		}
> 
> 		/* Did somebody else fill it already? */
> 		if (PageUptodate(page)) {
> 			unlock_page(page);
> 			goto page_ok;
> 		}
> 
> readpage:
> 		/*
> 		 * A previous I/O error may have been due to temporary
> 		 * failures, eg. multipath errors.
> 		 * PG_error will be set again if readpage fails.
> 		 */
> 		ClearPageError(page);
> 		/* Start the actual read. The read will unlock the page. */
> 		error = mapping->a_ops->readpage(filp, page);
> 
> 		if (unlikely(error)) {
> 			if (error == AOP_TRUNCATED_PAGE) {
> 				put_page(page);
> 				error = 0;
> 				goto find_page;
> 			}
> 			goto readpage_error;
> 		}
> 
> 		if (!PageUptodate(page)) {
> 			error = lock_page_killable(page);
> 			if (unlikely(error))
> 				goto readpage_error;
> 			if (!PageUptodate(page)) {
> 				if (page->mapping == NULL) {
> 					/*
> 					 * invalidate_mapping_pages got it
> 					 */
> 					unlock_page(page);
> 					put_page(page);
> 					goto find_page;
> 				}
> 				unlock_page(page);
> 				shrink_readahead_size_eio(filp, ra);
> 				error = -EIO;
> 				goto readpage_error;
> 			}
> 			unlock_page(page);
> 		}
> 
> 		goto page_ok;
> 
> readpage_error:
> 		/* UHHUH! A synchronous read error occurred. Report it */
> 		put_page(page);
> 		goto out;
> 
> no_cached_page:
> 		/*
> 		 * Ok, it wasn't cached, so we need to create a new
> 		 * page..
> 		 */
> 		page = page_cache_alloc(mapping);
> 		if (!page) {
> 			error = -ENOMEM;
> 			goto out;
> 		}
> 		error = add_to_page_cache_lru(page, mapping, index,
> 				mapping_gfp_constraint(mapping, GFP_KERNEL));
> 		if (error) {
> 			put_page(page);
> 			if (error == -EEXIST) {
> 				error = 0;
> 				goto find_page;
> 			}
> 			goto out;
> 		}
> 		goto readpage;
> 	}
> 
> would_block:
> 	error = -EAGAIN;
> out:
> 	ra->prev_pos = prev_index;
> 	ra->prev_pos <<= PAGE_SHIFT;
> 	ra->prev_pos |= prev_offset;
> 
> 	*ppos = ((loff_t)index << PAGE_SHIFT) + offset;
> 	file_accessed(filp);
> 	return written ? written : error;
> }
> 
> /**
>  * ecryptfs_file_buffered_read2 - simplified ecryptfs file read routine
>  * @iocb:	the iocb to read
>  * @iter:	data destination
>  * @written:	already copied
>  *
>  * This is a generic file read routine, and uses the
>  * mapping->a_ops->readpage() function for the actual low-level stuff.
>  *
>  * This is really ugly. But the goto's actually try to clarify some
>  * of the logic when it comes to error handling etc.
>  */
> ssize_t ecryptfs_file_buffered_read2(struct kiocb *iocb,
> 		struct iov_iter *iter, ssize_t written)
> {
> 	struct file *filp = iocb->ki_filp;
> 	struct address_space *mapping = filp->f_mapping;
> 	struct inode *inode = mapping->host;
> 	struct file_ra_state *ra = &filp->f_ra;
> 	loff_t *ppos = &iocb->ki_pos;
> 	pgoff_t index;
> 	pgoff_t last_index;
> 	pgoff_t prev_index;
> 	unsigned long offset;      /* offset into pagecache page */
> 	unsigned int prev_offset;
> 	int error = 0;
> 	struct page **pgs_cached, **pgs_no_cached;
> 	int nr_pgs;
> 	int i = 0, pg_idx = 0, nr_pgs_no_cached = 0, nr_pgs_cached = 0;
> 	loff_t isize = i_size_read(inode);
>     size_t real_count = iter->count;
> 
>     if (unlikely(*ppos >= i_size_read(inode)))
>         return 0;
> 	if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
> 		return 0;
> 	iov_iter_truncate(iter, inode->i_sb->s_maxbytes);
> 
>     if (i_size_read(inode) - *ppos < real_count)
>         real_count = i_size_read(inode) - *ppos;
>     nr_pgs = DIV_ROUND_UP(real_count, PAGE_SIZE);
> 	pgs_cached = kzalloc(nr_pgs * sizeof(struct page *), GFP_KERNEL);
> 	pgs_no_cached = kzalloc(nr_pgs * sizeof(struct page *), GFP_KERNEL);
> 	if (!pgs_cached || !pgs_no_cached) {
> 	    error = -ENOMEM;
> 	    printk(KERN_ERR "[kava] Error allocating pages\n");
> 	    goto out;
> 	}
> 
> 	index = *ppos >> PAGE_SHIFT;
> 	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
> 
> 	for (; pg_idx < nr_pgs;) {
> 		struct page *page;
> 
> 		cond_resched();
> find_page:
> 		if (fatal_signal_pending(current)) {
> 			error = -EINTR;
> 			goto out;
> 		}
> 
> 		page = find_get_page(mapping, index);
> 		if (!page) {
> 			if (iocb->ki_flags & IOCB_NOWAIT)
> 				goto would_block;
> 			page_cache_sync_readahead(mapping,
> 					ra, filp,
> 					index, last_index - index);
> 			page = find_get_page(mapping, index);
> 			if (unlikely(page == NULL))
> 				goto no_cached_page;
> 		}
> 		if (PageReadahead(page)) {
> 			page_cache_async_readahead(mapping,
> 					ra, filp, page,
> 					index, last_index - index);
> 		}
> 
>         pgs_cached[pg_idx++] = page;
>         index++;
>         nr_pgs_cached++;
> 		continue;
> 
> no_cached_page:
> 		/*
> 		 * Ok, it wasn't cached, so we need to create a new
> 		 * page..
> 		 */
> 		page = page_cache_alloc(mapping);
> 		if (!page) {
> 			error = -ENOMEM;
> 			goto out;
> 		}
> 		error = add_to_page_cache_lru(page, mapping, index,
> 				mapping_gfp_constraint(mapping, GFP_KERNEL));
> 		if (error) {
> 			put_page(page);
> 			if (error == -EEXIST) {
> 				error = 0;
> 				goto find_page;
> 			}
> 			goto out;
> 		}
> 
>         /*
>          * A previous I/O error may have been due to temporary
>          * failures, eg. multipath errors.
>          * PG_error will be set again if readpage fails.
>          */
>         ClearPageError(page);
> 
>         pgs_no_cached[nr_pgs_no_cached++] = page;
>         pg_idx++;
>         index++;
> 	}
> 
> readpage:
>     /* Start the actual read. The read will unlock the page. */
>     //pr_info("nr_pgs_no_cached = %x, nr_pgs_cached = %x\n", nr_pgs_no_cached, nr_pgs_cached);
>     //error = mapping->a_ops->readpages(filp, mapping, pgs_no_cached, nr_pgs_no_cached);
>     if (nr_pgs_no_cached)
>         error = ecryptfs_decrypt_pages(pgs_no_cached, nr_pgs_no_cached);
> 
>     if (unlikely(error))
>         goto readpage_error;
> 
> page_ok:
> 	index = *ppos >> PAGE_SHIFT;
> 	prev_index = ra->prev_pos >> PAGE_SHIFT;
> 	prev_offset = ra->prev_pos & (PAGE_SIZE-1);
> 	offset = *ppos & ~PAGE_MASK;
> 
>     pg_idx = 0;
>     i = 0;
> 
>     for (; pg_idx < nr_pgs;) {
>         struct page *page;
> 		pgoff_t end_index;
> 		loff_t isize;
> 		unsigned long nr, ret;
> 
>         page = pgs_cached[pg_idx];
>         pgs_cached[pg_idx++] = NULL;
>         if (likely(!page)) {
>             page = pgs_no_cached[i];
>             pgs_no_cached[i++] = NULL;
> 
>             if (unlikely(!page)) {
>                 error = -EEXIST;
>                 goto out;
>             }
> 
>             if (!PageUptodate(page)) {
>                 error = lock_page_killable(page);
>                 if (unlikely(error))
>                     goto readpage_error;
>                 if (!PageUptodate(page)) {
>                     if (page->mapping == NULL) {
>                         /*
>                          * invalidate_mapping_pages got it
>                          */
>                         unlock_page(page);
>                         put_page(page);
>                         error = -EIO;
>                         goto readpage_error;
>                     }
>                     unlock_page(page);
>                     shrink_readahead_size_eio(filp, ra);
>                     error = -EIO;
>                     goto readpage_error;
>                 }
>                 unlock_page(page);
>             }
>         }
> 
> 		/*
> 		 * i_size must be checked after we know the page is Uptodate.
> 		 *
> 		 * Checking i_size after the check allows us to calculate
> 		 * the correct value for "nr", which means the zero-filled
> 		 * part of the page is not copied back to userspace (unless
> 		 * another truncate extends the file - this is desired though).
> 		 */
> 
> 		isize = i_size_read(inode);
> 		end_index = (isize - 1) >> PAGE_SHIFT;
> 		if (unlikely(!isize || index > end_index)) {
> 			put_page(page);
> 			goto out;
> 		}
> 
> 		/* nr is the maximum number of bytes to copy from this page */
> 		nr = PAGE_SIZE;
> 		if (index == end_index) {
> 			nr = ((isize - 1) & ~PAGE_MASK) + 1;
> 			if (nr <= offset) {
> 				put_page(page);
> 				goto out;
> 			}
> 		}
> 		nr = nr - offset;
> 
> 		/* If users can be writing to this page using arbitrary
> 		 * virtual addresses, take care about potential aliasing
> 		 * before reading the page on the kernel side.
> 		 */
> 		if (mapping_writably_mapped(mapping))
> 			flush_dcache_page(page);
> 
> 		/*
> 		 * When a sequential read accesses a page several times,
> 		 * only mark it as accessed the first time.
> 		 */
> 		if (prev_index != index || offset != prev_offset)
> 			mark_page_accessed(page);
> 		prev_index = index;
> 
> 		/*
> 		 * Ok, we have the page, and it's up-to-date, so
> 		 * now we can copy it to user space...
> 		 */
> 
> 		ret = copy_page_to_iter(page, offset, nr, iter);
> 		offset += ret;
> 		index += offset >> PAGE_SHIFT;
> 		offset &= ~PAGE_MASK;
> 		prev_offset = offset;
> 
> 		put_page(page);
> 		written += ret;
> 		if (!iov_iter_count(iter))
> 			goto out;
> 		if (ret < nr) {
> 			error = -EFAULT;
> 			goto out;
> 		}
>     }
> 
> readpage_error:
> 	/* UHHUH! A synchronous read error occurred. Report it */
> 	goto out;
> 
> would_block:
> 	error = -EAGAIN;
> out:
>     for (i = 0; i < nr_pgs; i++) {
>         if (pgs_cached[i])
>             put_page(pgs_cached[i]);
>         if (pgs_no_cached[i])
>             put_page(pgs_no_cached[i]);
>     }
> 
>     if (nr_pgs > 0) {
>         kfree(pgs_cached);
>         kfree(pgs_no_cached);
>     }
> 
> 	ra->prev_pos = prev_index;
> 	ra->prev_pos <<= PAGE_SHIFT;
> 	ra->prev_pos |= prev_offset;
> 
> 	*ppos = ((loff_t)index << PAGE_SHIFT) + offset;
> 	file_accessed(filp);
> 	return written ? written : error;
Only in ecryptfs: run.sh
Only in ecryptfs: unload.sh
